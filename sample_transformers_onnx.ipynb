{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZgdtZPPmzmQ",
        "outputId": "4f3d69f9-657a-4614-a883-d648b063abdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 48.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 31.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 2.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.21.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.13.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 12.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=620a4c257916e3982eac2407025533ba436ef07ce3316d9558d0455d4a43afcd\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.97\n"
          ]
        }
      ],
      "source": [
        "#lets convert a popular bert similairty embedding pre train models\n",
        "\n",
        "\n",
        "!pip install transformers\n",
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zA-CMML3wo-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer,util\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    print(token_embeddings)\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "\n",
        "# Load model from HuggingFace Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L12-v2')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L12-v2')\n",
        "\n",
        "#"
      ],
      "metadata": {
        "id": "PtW8BzAV-eEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentences we want sentence embeddings for\n",
        "sentences = ['This is an example sentence', 'This is sample of the sentence']\n",
        "\n",
        "\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "\n",
        "\n",
        "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Compute token embeddings\n",
        "with torch.no_grad():\n",
        "    model_output = model(**encoded_input)\n",
        "\n",
        "# Perform pooling\n",
        "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "\n",
        "# Normalize embeddings\n",
        "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
        "cosine_scores = util.pytorch_cos_sim(sentence_embeddings[0], sentence_embeddings[1])\n",
        "cosine_scores\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(end - start)\n",
        "print(f\"pytorch vanilla cpu: {(end- start)/2:.2f}s/sequence\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XPvRSDMw31S",
        "outputId": "b5c0e5b7-ee8b-4493-89fc-0c3dfade4d18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.0519, -0.0301,  0.2138,  ...,  0.1786,  0.0573, -0.0119],\n",
            "         [-0.1315,  0.6603,  0.1879,  ...,  0.1245,  0.7793,  0.3559],\n",
            "         [-0.3818,  0.8268, -0.1493,  ...,  0.5605,  0.6444,  0.0144],\n",
            "         ...,\n",
            "         [ 0.3050,  0.1931,  0.5416,  ...,  0.2212, -0.1725, -1.3564],\n",
            "         [ 0.0519, -0.0301,  0.2138,  ...,  0.1786,  0.0573, -0.0119],\n",
            "         [ 0.0519, -0.0301,  0.2138,  ...,  0.1786,  0.0573, -0.0119]],\n",
            "\n",
            "        [[ 0.1090,  0.1189,  0.0785,  ...,  0.1729, -0.0939,  0.1119],\n",
            "         [-0.0034,  1.0699,  0.3069,  ...,  0.0838,  0.6371,  0.5188],\n",
            "         [ 0.0979,  0.6687, -0.0734,  ...,  0.3327,  0.6669,  0.1522],\n",
            "         ...,\n",
            "         [ 0.2591,  0.7813, -0.0958,  ...,  0.0458,  0.2115,  0.4994],\n",
            "         [ 0.3489,  0.2220,  0.3398,  ...,  0.0702, -0.4704, -1.0491],\n",
            "         [ 0.1091,  0.1189,  0.0785,  ...,  0.1729, -0.0939,  0.1119]]])\n",
            "0.04085946083068848\n",
            "pytorch vanilla cpu: 0.02s/sequence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9XVywWLZLGIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.onnx.export(\n",
        "    model, \n",
        "    tuple(encoded_input.values()),\n",
        "    f=\"torch-model.onnx\",  \n",
        "    input_names=['input_ids', 'attention_mask','token_type_ids'], \n",
        "    output_names=['logits'], \n",
        "    dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence'}, \n",
        "                  'attention_mask': {0: 'batch_size', 1: 'sequence'}, \n",
        "                  'token_type_ids': {0: 'batch_size', 1: 'sequence'},\n",
        "                  'logits': {0: 'batch_size', 1: 'sequence'}}, \n",
        "    do_constant_folding=True, \n",
        "    opset_version=13, \n",
        ")"
      ],
      "metadata": {
        "id": "Ui0UksNGx7dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime \n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "JJHEueBSHePJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime\n",
        "import time\n",
        "ort_session = onnxruntime.InferenceSession(\"torch-model.onnx\", providers=[\"CPUExecutionProvider\"])\n",
        "\n",
        "def to_numpy(tensor):\n",
        "    return tensor.detach.cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
        "\n",
        "def run_inference(input):\n",
        "  tokenei= tokenizer(input, padding=True, truncation=True,return_tensors=\"pt\")\n",
        "  attention_mask = tokenei['attention_mask']\n",
        "  tokenei['input_ids'] =[to_numpy(x) for x in tokenei['input_ids']]\n",
        "  tokenei['attention_mask'] =[to_numpy(x) for x in tokenei['attention_mask']]\n",
        "  tokenei['token_type_ids'] =[to_numpy(x) for x in tokenei['token_type_ids']]\n",
        "  ort_outs = ort_session.run(['logits'], dict(tokenei))\n",
        "\n",
        "  \n",
        "  return (ort_outs), attention_mask\n",
        "\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "output,attention_mask = run_inference(sentences)\n",
        "\n",
        "red = torch.Tensor(output)\n",
        "# Perform pooling\n",
        "sentence_embeddings = mean_pooling(model_output, attention_mask)\n",
        "\n",
        "# Normalize embeddings\n",
        "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
        "cosine_scores = util.pytorch_cos_sim(sentence_embeddings[0], sentence_embeddings[1])\n",
        "cosine_scores\n",
        "\n",
        "\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(end - start)\n",
        "print(f\"onnx cpu: {(end- start)/2:.2f}s/sequence\")"
      ],
      "metadata": {
        "id": "zxa1_WxdGPJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MMxjBXZ3JytM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "\n",
        "output,attention_mask = run_inference(sentences)\n",
        "\n",
        "red = torch.Tensor(output)\n",
        "# Perform pooling\n",
        "sentence_embeddings = mean_pooling(model_output, attention_mask)\n",
        "\n",
        "# Normalize embeddings\n",
        "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
        "cosine_scores = util.pytorch_cos_sim(sentence_embeddings[0], sentence_embeddings[1])\n",
        "cosine_scores\n",
        "\n",
        "\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(end - start)\n",
        "print(f\"onnx cpu: {(end- start)/2:.2f}s/sequence\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG5kOOxJJ2us",
        "outputId": "5ae25546-5c0b-4d83-83cd-fa9c6ee344a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.0519, -0.0301,  0.2138,  ...,  0.1786,  0.0573, -0.0119],\n",
            "         [-0.1315,  0.6603,  0.1879,  ...,  0.1245,  0.7793,  0.3559],\n",
            "         [-0.3818,  0.8268, -0.1493,  ...,  0.5605,  0.6444,  0.0144],\n",
            "         ...,\n",
            "         [ 0.3050,  0.1931,  0.5416,  ...,  0.2212, -0.1725, -1.3564],\n",
            "         [ 0.0519, -0.0301,  0.2138,  ...,  0.1786,  0.0573, -0.0119],\n",
            "         [ 0.0519, -0.0301,  0.2138,  ...,  0.1786,  0.0573, -0.0119]],\n",
            "\n",
            "        [[ 0.1090,  0.1189,  0.0785,  ...,  0.1729, -0.0939,  0.1119],\n",
            "         [-0.0034,  1.0699,  0.3069,  ...,  0.0838,  0.6371,  0.5188],\n",
            "         [ 0.0979,  0.6687, -0.0734,  ...,  0.3327,  0.6669,  0.1522],\n",
            "         ...,\n",
            "         [ 0.2591,  0.7813, -0.0958,  ...,  0.0458,  0.2115,  0.4994],\n",
            "         [ 0.3489,  0.2220,  0.3398,  ...,  0.0702, -0.4704, -1.0491],\n",
            "         [ 0.1091,  0.1189,  0.0785,  ...,  0.1729, -0.0939,  0.1119]]])\n",
            "0.029510021209716797\n",
            "onnx cpu: 0.01s/sequence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#onnx optimization\n",
        "\n",
        "\n",
        "!pip3 install onnxoptimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uH9n5g0NNZyj",
        "outputId": "28e8c0f1-6a18-46b3-9d2a-34deb1083d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting onnxoptimizer\n",
            "  Downloading onnxoptimizer-0.3.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (602 kB)\n",
            "\u001b[K     |████████████████████████████████| 602 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting onnx\n",
            "  Downloading onnx-1.12.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1 MB 43.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx->onnxoptimizer) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx->onnxoptimizer) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnx->onnxoptimizer) (1.21.6)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1,>=3.12.2->onnx->onnxoptimizer) (1.15.0)\n",
            "Installing collected packages: onnx, onnxoptimizer\n",
            "Successfully installed onnx-1.12.0 onnxoptimizer-0.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "onnx optimization"
      ],
      "metadata": {
        "id": "KjmtLIUUhtNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "model_fp32 = 'torch-model.onnx'\n",
        "model_quant = 'model.quant.onnx'\n",
        "quantized_model = quantize_dynamic(model_fp32, model_quant)"
      ],
      "metadata": {
        "id": "3drc2FhvXi8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime\n",
        "import time\n",
        "ort_session = onnxruntime.InferenceSession(\"model.quant.onnx\", providers=[\"CPUExecutionProvider\"])\n",
        "\n",
        "def to_numpy(tensor):\n",
        "    return tensor.detach.cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
        "\n",
        "def run_inference(input):\n",
        "  tokenei= tokenizer(input, padding=True, truncation=True,return_tensors=\"pt\")\n",
        "  attention_mask = tokenei['attention_mask']\n",
        "  tokenei['input_ids'] =[to_numpy(x) for x in tokenei['input_ids']]\n",
        "  tokenei['attention_mask'] =[to_numpy(x) for x in tokenei['attention_mask']]\n",
        "  tokenei['token_type_ids'] =[to_numpy(x) for x in tokenei['token_type_ids']]\n",
        "  ort_outs = ort_session.run(['logits'], dict(tokenei))\n",
        "\n",
        "  \n",
        "  return (ort_outs), attention_mask\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "output,attention_mask = run_inference(sentences)\n",
        "\n",
        "red = torch.Tensor(output)\n",
        "# Perform pooling\n",
        "sentence_embeddings = mean_pooling(model_output, attention_mask)\n",
        "\n",
        "# Normalize embeddings\n",
        "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
        "cosine_scores = util.pytorch_cos_sim(sentence_embeddings[0], sentence_embeddings[1])\n",
        "cosine_scores\n",
        "\n",
        "\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(end - start)\n",
        "print(f\"onnx cpu: {(end- start)/2:.2f}s/sequence\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fg5heDkAgyfw",
        "outputId": "5e0a6549-c963-4925-e62b-739bd478a876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.0519, -0.0301,  0.2138,  ...,  0.1786,  0.0573, -0.0119],\n",
            "         [-0.1315,  0.6603,  0.1879,  ...,  0.1245,  0.7793,  0.3559],\n",
            "         [-0.3818,  0.8268, -0.1493,  ...,  0.5605,  0.6444,  0.0144],\n",
            "         ...,\n",
            "         [ 0.3050,  0.1931,  0.5416,  ...,  0.2212, -0.1725, -1.3564],\n",
            "         [ 0.0519, -0.0301,  0.2138,  ...,  0.1786,  0.0573, -0.0119],\n",
            "         [ 0.0519, -0.0301,  0.2138,  ...,  0.1786,  0.0573, -0.0119]],\n",
            "\n",
            "        [[ 0.1090,  0.1189,  0.0785,  ...,  0.1729, -0.0939,  0.1119],\n",
            "         [-0.0034,  1.0699,  0.3069,  ...,  0.0838,  0.6371,  0.5188],\n",
            "         [ 0.0979,  0.6687, -0.0734,  ...,  0.3327,  0.6669,  0.1522],\n",
            "         ...,\n",
            "         [ 0.2591,  0.7813, -0.0958,  ...,  0.0458,  0.2115,  0.4994],\n",
            "         [ 0.3489,  0.2220,  0.3398,  ...,  0.0702, -0.4704, -1.0491],\n",
            "         [ 0.1091,  0.1189,  0.0785,  ...,  0.1729, -0.0939,  0.1119]]])\n",
            "0.01838827133178711\n",
            "onnx cpu: 0.01s/sequence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KLBDvbFkhpKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Onnx graph optmization"
      ],
      "metadata": {
        "id": "iqqf9HhIhpfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sess_options = onnxruntime.SessionOptions()\n",
        "\n",
        "# Set graph optimization level\n",
        "sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
        "\n",
        "# To enable model serialization after graph optimization set this\n",
        "sess_options.optimized_model_filepath = \"optimizegraphmodel.onnx\"\n",
        "\n",
        "ort_session = onnxruntime.InferenceSession(\"torch-model.onnx\", sess_options,providers=[\"CPUExecutionProvider\"])\n",
        "\n",
        "def to_numpy(tensor):\n",
        "    return tensor.detach.cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
        "\n",
        "def run_inference(input):\n",
        "  tokenei= tokenizer(input, padding=True, truncation=True,return_tensors=\"pt\")\n",
        "  attention_mask = tokenei['attention_mask']\n",
        "  tokenei['input_ids'] =[to_numpy(x) for x in tokenei['input_ids']]\n",
        "  tokenei['attention_mask'] =[to_numpy(x) for x in tokenei['attention_mask']]\n",
        "  tokenei['token_type_ids'] =[to_numpy(x) for x in tokenei['token_type_ids']]\n",
        "  ort_outs = ort_session.run(['logits'], dict(tokenei))\n",
        "\n",
        "  \n",
        "  return (ort_outs), attention_mask\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "output,attention_mask = run_inference(sentences)\n",
        "\n",
        "red = torch.Tensor(output)\n",
        "# Perform pooling\n",
        "sentence_embeddings = mean_pooling(model_output, attention_mask)\n",
        "\n",
        "# Normalize embeddings\n",
        "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
        "cosine_scores = util.pytorch_cos_sim(sentence_embeddings[0], sentence_embeddings[1])\n",
        "cosine_scores\n",
        "\n",
        "\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(end - start)\n",
        "print(f\"onnx cpu: {(end- start)/2:.2f}s/sequence\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiCqaFHzho8j",
        "outputId": "4d559e2d-6818-47bc-b59b-9c8910f3de47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.0519, -0.0301,  0.2138,  ...,  0.1786,  0.0573, -0.0119],\n",
            "         [-0.1315,  0.6603,  0.1879,  ...,  0.1245,  0.7793,  0.3559],\n",
            "         [-0.3818,  0.8268, -0.1493,  ...,  0.5605,  0.6444,  0.0144],\n",
            "         ...,\n",
            "         [ 0.3050,  0.1931,  0.5416,  ...,  0.2212, -0.1725, -1.3564],\n",
            "         [ 0.0519, -0.0301,  0.2138,  ...,  0.1786,  0.0573, -0.0119],\n",
            "         [ 0.0519, -0.0301,  0.2138,  ...,  0.1786,  0.0573, -0.0119]],\n",
            "\n",
            "        [[ 0.1090,  0.1189,  0.0785,  ...,  0.1729, -0.0939,  0.1119],\n",
            "         [-0.0034,  1.0699,  0.3069,  ...,  0.0838,  0.6371,  0.5188],\n",
            "         [ 0.0979,  0.6687, -0.0734,  ...,  0.3327,  0.6669,  0.1522],\n",
            "         ...,\n",
            "         [ 0.2591,  0.7813, -0.0958,  ...,  0.0458,  0.2115,  0.4994],\n",
            "         [ 0.3489,  0.2220,  0.3398,  ...,  0.0702, -0.4704, -1.0491],\n",
            "         [ 0.1091,  0.1189,  0.0785,  ...,  0.1729, -0.0939,  0.1119]]])\n",
            "0.03601384162902832\n",
            "onnx cpu: 0.02s/sequence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nehWFQn2i8uD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}